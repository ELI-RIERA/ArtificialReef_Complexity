{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Unleashing the Potential of Artificial Reefs Design**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script aims at extracting parmaters and elements from 3D CAD models to compute complexity indexes. It is based on **STL** files. \n",
    "\n",
    "**Section I : Extraction of the parameters of the 3D CAD models**\n",
    "* Computation of the convexhull of the mesh, and their respective area and volume\n",
    "* Extraction of point clouds and normals to be saved as txt files\n",
    "* Computation of the sum of normals and the number of different normals for each 3D CAD models\n",
    "\n",
    "**Section II : Computation of geometrical and informational complexity indexes**\n",
    "* Computation of Packing (P) and Convexity (C) (fractal dimension is computed on R, script available at: )\n",
    "* Computation of Richness (R), Diversity (D) and Evenness (J) on normals \n",
    "\n",
    "For more details refer to the paper available at:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy as np\n",
    "from numpy import savetxt\n",
    "import pandas as pd\n",
    "import trimesh\n",
    "from scipy.spatial import ConvexHull\n",
    "from scipy.stats import entropy\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download STL file from Zenodo\n",
    "\n",
    "As an example to test the script, 3D CAD mesh models (STL files) are available on Zenodo:\n",
    "\n",
    "[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.8048071.svg)](https://doi.org/10.5281/zenodo.8048071)\n",
    "\n",
    "Otherwise use your own mesh at the format STL "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Path directory & folders\n",
    "\n",
    "Mesh (STL files):\n",
    "* Create a folder where you store your mesh (STL files) `data_mesh` or the one that you downloaded from Zenodo\n",
    "* Determine the path of your mesh `data_mesh_path`\n",
    "\n",
    "Point clouds and normals (txt files):\n",
    "* Create two folders where you'll store the point clouds `data_point_clouds` and the normals `data_normals`\n",
    "* Determine the path directory of these folders\n",
    "\n",
    "outputs:\n",
    "* Create a folder `output` where you'll store the output from the difference computation process\n",
    "* Determine the path of the output folder `output_path`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_mesh_path = \"./data_STL\"\n",
    "data_point_clouds_path = \"./data_point_clouds\"\n",
    "data_normals_path = \"./data_normals\"\n",
    "output_path = \"./output\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Create a list of your mesh `list_mesh`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_mesh = os.listdir(data_mesh_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Extraction of the parameters of the 3D CAD models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I.1 Area and Volume of Mesh and its ConvexHull"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section: \n",
    "* Computation of the convexhull of each mesh of 3D CAD models. The convexhull of a shape is the smallest convex set that contains it.\n",
    "* Computation of the area and volume of all mesh and its respective convexhull that you save in a dataframe called `AreaVol_parameters`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Prepare the loop for the computation \n",
    "\n",
    "* Create an empty object `AreaVol_parameters` where area and volume parameters will be stored durign the loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AreaVol_parameters = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Launch the loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(list_mesh)):\n",
    "  \n",
    "  # Load mesh files\n",
    "  print(\"--- {} processing {} ---\".format(pd.Timestamp.now(), list_mesh[i]))\n",
    "  mesh = trimesh.load(os.path.join(data_mesh_path, list_mesh[i]), updateNormals=True, readcolor=False,\n",
    "                        clean=True, silent=False)\n",
    "\n",
    "  ### COMPUTE THE CONVEXHULL \n",
    "\n",
    "  # Extract coordinate of vertices to compute the convexhull\n",
    "  points = mesh.vertices\n",
    "  hull = ConvexHull(points)\n",
    "\n",
    "  # remove unused vertices from the convex hull\n",
    "  used_vertices = np.unique(hull.simplices.flatten())\n",
    "  vertices = points[used_vertices]\n",
    "  faces = np.searchsorted(used_vertices, hull.simplices)\n",
    "  convex_mesh = trimesh.Trimesh(vertices=vertices, faces=faces)\n",
    "  convex_mesh.fix_normals() \n",
    "    \n",
    "  ### COMPUTE AREA & VOLUME of the MESH & CONVEXHULL\n",
    "\n",
    "  mesh_area = mesh.area \n",
    "  mesh_volume = mesh.volume\n",
    "  convex_mesh_area = convex_mesh.area\n",
    "  convex_mesh_volume = convex_mesh.volume\n",
    "\n",
    "  ### APPEND `AreaVol_parameters` TO LIST\n",
    "  file_names = [os.path.splitext(file)[0] for file in list_mesh]\n",
    "  AreaVol_parameters.append((file_names[i], mesh_area, mesh_volume, \n",
    "                     convex_mesh_area, convex_mesh_volume))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I.2 Extraction of Point clouds and normals from the Mesh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section: \n",
    "* extraction of point clouds from the mesh that you save as `.txt` file for fractal dimension computation\n",
    "* extraction of normals associated to the point clouds \n",
    "* computation the number of normals and the number of different normals that you save in a dataframe `normals_parameters`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Prepare the loop for the computation\n",
    "\n",
    "* Create an empty object `normals_parameters` where normals will be stored during the loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normals_parameters = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Launch the loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(list_mesh)):\n",
    " print(\"--- {} processing {} ---\".format(pd.Timestamp.now(), list_mesh[i]))\n",
    " mesh = trimesh.load(os.path.join(data_mesh_path, list_mesh[i]), updateNormals=True, readcolor=False,\n",
    "                        clean=True, silent=False)\n",
    "\n",
    " \n",
    " #### POINT CLOUDS EXTRACTION\n",
    "\n",
    " density = 0.01  # points per square centimeter\n",
    " point_count = int(mesh.area * density)\n",
    " point_cloud, index = trimesh.sample.sample_surface(mesh, count=point_count)\n",
    " point_cloud = point_cloud.view(np.ndarray) \n",
    " savetxt(os.path.join(data_point_clouds_path, file_names[i] + \".txt\"), point_cloud, delimiter = \",\")   \n",
    "\n",
    " \n",
    " #### NORMALS EXTRACTION\n",
    "\n",
    " ## Compute the normal and its amount\n",
    " x_y_z_cos = mesh.face_normals[index]\n",
    " normal = np.sum(x_y_z_cos, axis=1) \n",
    " nb_normal = len(normal)\n",
    "\n",
    " # Summing up the different type of normal orientation & save this data for further indexes computation\n",
    " sum_different_normal = pd.DataFrame(normal).apply(pd.value_counts).fillna(0).T\n",
    " file_names = [os.path.splitext(file)[0] for file in list_mesh]\n",
    " sum_different_normal.to_csv(os.path.join(data_normals_path, file_names[i] + \".csv\"), index=False)\n",
    "    \n",
    " # Count of the different normal vectors \n",
    " nb_different_normal = sum_different_normal.astype(bool).sum(axis=1)\n",
    " \n",
    " ### APPEND `normals_parameters` TO LIST\n",
    " file_names = [os.path.splitext(file)[0] for file in list_mesh]\n",
    " normals_parameters.append((file_names[i],\n",
    "                    nb_normal, nb_different_normal.iloc[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I.3. Save the paramaters data\n",
    "\n",
    "* Concatenate `normals_parameters` and `AreaVol_parameters` into a panda dataframe `df_parameters`\n",
    "* Rename the rows and columns\n",
    "* Verify your dataframe\n",
    "* Save it to the `output_path`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_parameters = pd.concat([pd.DataFrame(AreaVol_parameters).iloc[:, 1:], pd.DataFrame(normals_parameters).iloc[:, 1:]], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_parameters.index = file_names\n",
    "df_parameters.columns = [\"mesh_area\", \"mesh_volume\", \n",
    "                        \"convex_mesh_area\", \"convex_mesh_volume\",\n",
    "                        \"nb_normal\",\"nb_different_normal\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_parameters.to_csv(os.path.join(output_path, \"df_parameters.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II. Computation of geometrical and informational complexity indexes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II.1 Geometrical complexity : Paking (P) & Convexity (C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section:\n",
    "* computation of Packing (P): measure of the degree of space between different parts of an object.\n",
    "* computation of Convexity (C): measure of the degree of space available between different parts of an object.\n",
    "* computation of Fractal Dimension (D): R script available on github ()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Compute paking and Comvexity from the volume and area of the mesh and its convexhull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = ((df_parameters['convex_mesh_volume']-df_parameters['mesh_volume'])/df_parameters['convex_mesh_volume'])\n",
    "C = C.tolist()\n",
    "\n",
    "P = 1-(df_parameters['convex_mesh_area']/df_parameters['mesh_area'])\n",
    "P = P.tolist() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II.2 Informational complexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section:\n",
    "* computation of Orientation Richness (R): measure the proportion of the different orientation of the normals.\n",
    "* computation of Orientation Diversity (H): measure the diversity of the orientation of the normals.\n",
    "* computation of Orientation Evenness (J): measure the evenness of the orientation of the normals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Orientation richness (R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R = df_parameters['nb_different_normal']/df_parameters['nb_normal']\n",
    "R = R.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Orientation diversity (H) & Orientation evenness (J)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Prepare the loop for the computation of H and J\n",
    "\n",
    "* Create a list from the normals files computed previously\n",
    "* Create an empty objects `H` & `J` where indexes will be stored durign the loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_normals = os.listdir(data_normals_path)\n",
    "log_H = []\n",
    "diversity = []\n",
    "evenness = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Launch the loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(list_normals)):\n",
    "  \n",
    " print(\"--- {} processing {} ---\".format(pd.Timestamp.now(), list_normals[i]))\n",
    "\n",
    " normals = pd.read_csv(os.path.join(data_normals_path, list_normals[i]))\n",
    "    \n",
    " H = float(np.asarray(entropy(normals,axis=1)))\n",
    "\n",
    " J = H / np.log(len(normals.axes[1]))\n",
    "    \n",
    " H_log = np.log(1+H)\n",
    "    \n",
    " diversity.append(H)\n",
    " log_H.append(H_log)\n",
    " evenness.append(J)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. Save the data computed\n",
    "\n",
    "* Concatenate your indexes `R, H, J, P, C` into a panda dataframe\n",
    "* Rename the rows and columns\n",
    "* Verify your dataframe\n",
    "* Save it to the output path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_complexity_indexes = pd.DataFrame({'R': R, 'H': log_H, 'J': evenness, 'P': P, 'C': C}).round(3)\n",
    "df_complexity_indexes.index = file_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_complexity_indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_complexity_indexes.to_csv(os.path.join(output_path,\"df_complexity_indexes.csv\"), index=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
